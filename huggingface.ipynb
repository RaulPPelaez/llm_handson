{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa9e0bb-383d-47bb-852b-09e435500dae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "def display_chat(messages):\n",
    "    \"\"\"\n",
    "    Displays messages in a Jupyter Notebook using Markdown formatting.\n",
    "    Different roles ('system', 'user', 'assistant') are styled differently.\n",
    "    \"\"\"\n",
    "\n",
    "    markdown_output = \"\"\n",
    "\n",
    "    for message in messages:\n",
    "        role = message.get('role')\n",
    "        content = message.get('content').replace('\\n', '  \\n')\n",
    "        if role is None or content is None:\n",
    "            raise ValueError(\"Each message must have 'role' and 'content'.\")\n",
    "        if role == 'system':\n",
    "            markdown_output += f\"**System prompt:** {content}\\n\\n\"\n",
    "        elif role == 'user':\n",
    "            markdown_output += f\"ðŸ‘¤: {content}\\n\\n\"\n",
    "        elif role == 'assistant':\n",
    "            markdown_output += f\"ðŸ¤–: {content}\\n\\n\"\n",
    "        else:\n",
    "            markdown_output += f\"Unrecognized role:{role}\\n\\n\"\n",
    "\n",
    "    # Display formatted markdown\n",
    "    display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a812b402-17b6-4177-b1c7-00ea0dcad346",
   "metadata": {},
   "source": [
    "# Model inference with HuggingFace transformers \n",
    "\n",
    "We will load a certain model from HuggingFace and run inference on it.\n",
    "\n",
    "The example models below are all finetuned versions of the first one, Mistral 7B. Which is a pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c016132-843b-4467-925c-f3b0a9197617",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc97afa4-4e43-4ee2-b7cb-50b7229654bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/raul/mambaforge/envs/llm_slides/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:21<00:00, 10.71s/it]\n",
      "/shared/raul/mambaforge/envs/llm_slides/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "#model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "#model_name = \"monology/openinstruct-mistral-7b\"\n",
    "model_name = \"openchat/openchat_3.5\"\n",
    "#model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# If the type is not specified to bfloat16, the model is loaded in float32 and takes twice the memory\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"cuda\", torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3948a92e-b9c6-41d0-87bb-595f78ecdac5",
   "metadata": {},
   "source": [
    "#### The model we got is \"just\" a regular pytorch nn.Model with a bunch of layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ce2cccd1-e1b3-403d-a42a-75546ccd0385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc0ec5-b1d8-4536-a1e2-8a6b64645953",
   "metadata": {},
   "source": [
    "## Prepare inputs for the model using the tokenizer\n",
    "\n",
    "LLMs expect \"tokens\" as inputs.\n",
    "A token is the number representation of a group of characters according to a \"vocabulary\".\n",
    "Might be a letter, symbol, word, emoji..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ef62b8d-4873-49f3-af65-9c5029c2c42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Vocabulary size: 32002"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "vocab = tokenizer.get_vocab()\n",
    "display(Markdown(f\"### Vocabulary size: {len(vocab)}\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1a66cd68-4ff4-41c5-8a79-b90c6af94d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## First tokens in the vocabulary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>: 0\n",
      "<s>: 1\n",
      "</s>: 2\n",
      "<0x00>: 3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Tokens for the first letters"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 28708\n",
      "b: 28726\n",
      "c: 28717\n",
      "d: 28715\n",
      "e: 28706\n",
      "f: 28722\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"## First tokens in the vocabulary\"))\n",
    "print(\"\\n\".join([f\"{k}: {v}\" for k,v in dict(sorted(vocab.items(), key=lambda k: k[1])).items()][:4]))\n",
    "display(Markdown(\"## Tokens for the first letters\"))\n",
    "print(\"\\n\".join([f\"{k}: {v}\" for k,v in dict(sorted(vocab.items())).items() if len(k)==1 and k in \"abcdef\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "286d31d0-ac40-40d3-a0df-cd9531688859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### **Original message**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**System prompt:** You are a helpful assistant.\n",
       "\n",
       "ðŸ‘¤: Translate 'The lazy dog' to spanish.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"### **Original message**:\"))\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"Translate 'The lazy dog' to spanish.\"\"\"},\n",
    "]\n",
    "display_chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af95a811-f98a-4d63-8f1d-7e9a0367f271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Some message is translated into a list of tokens"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### **Raw text**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>GPT4 Correct System: You are a helpful assistant.<|end_of_turn|>GPT4 Correct User: Translate 'The lazy dog' to spanish.<|end_of_turn|>GPT4 Correct Assistant:\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"## Some message is translated into a list of tokens\"))\n",
    "display(Markdown(\"### **Raw text**\"))\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c6b45899-2e1c-4a67-be92-701dfb898007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### **Tokens for the model**:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,   420,  6316, 28781,  3198,  3123,  2135, 28747,   995,   460,\n",
      "          264, 10865, 13892, 28723, 32000,   420,  6316, 28781,  3198,  3123,\n",
      "         1247, 28747,  4335, 10020,   464,  1014, 17898,  3914, 28742,   298,\n",
      "        12363,   789, 28723, 32000,   420,  6316, 28781,  3198,  3123, 21631,\n",
      "        28747], device='cuda:0')\n",
      "['<s>', 'G', 'PT', '4', 'Cor', 'rect', 'System', ':', 'You', 'are', 'a', 'helpful', 'assistant', '.', '<|end_of_turn|>', 'G', 'PT', '4', 'Cor', 'rect', 'User', ':', 'Trans', 'late', \"'\", 'The', 'lazy', 'dog', \"'\", 'to', 'span', 'ish', '.', '<|end_of_turn|>', 'G', 'PT', '4', 'Cor', 'rect', 'Assistant', ':']\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"### **Tokens for the model**:\"))\n",
    "inputs = (\n",
    "    tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, tokenize=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    .to(\"cuda\")\n",
    ")\n",
    "print(inputs[0])\n",
    "print([tokenizer.decode(s) for s in inputs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c2312c60-b596-4fcd-b4e9-2309643aee23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**System prompt:** You are a helpful assistant.\n",
       "\n",
       "ðŸ‘¤: Translate 'The lazy dog' to spanish.\n",
       "\n",
       "ðŸ¤–: The translation of 'The lazy dog' to Spanish is 'El perro perezoso'.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_tokens = model.generate(inputs)[0]\n",
    "response = tokenizer.decode(response_tokens[inputs.shape[-1] :], skip_special_tokens=True)\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "display_chat(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
