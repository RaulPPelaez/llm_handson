{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa9e0bb-383d-47bb-852b-09e435500dae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "def display_chat(messages):\n",
    "    \"\"\"\n",
    "    Displays messages in a Jupyter Notebook using Markdown formatting.\n",
    "    Different roles ('system', 'user', 'assistant') are styled differently.\n",
    "    \"\"\"\n",
    "\n",
    "    markdown_output = \"\"\n",
    "\n",
    "    for message in messages:\n",
    "        role = message.get('role')\n",
    "        content = message.get('content').replace('\\n', '  \\n')\n",
    "        if role is None or content is None:\n",
    "            raise ValueError(\"Each message must have 'role' and 'content'.\")\n",
    "        if role == 'system':\n",
    "            markdown_output += f\"**System prompt:** {content}\\n\\n\"\n",
    "        elif role == 'user':\n",
    "            markdown_output += f\"ðŸ‘¤: {content}\\n\\n\"\n",
    "        elif role == 'assistant':\n",
    "            markdown_output += f\"ðŸ¤–: {content}\\n\\n\"\n",
    "        else:\n",
    "            markdown_output += f\"Unrecognized role:{role}\\n\\n\"\n",
    "\n",
    "    # Display formatted markdown\n",
    "    display(Markdown(markdown_output))\n",
    "\n",
    "\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        # Retrieving GPU memory details\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "        reserved_memory = torch.cuda.memory_reserved(0)\n",
    "        allocated_memory = torch.cuda.memory_allocated(0)\n",
    "        free_memory = total_memory - reserved_memory\n",
    "\n",
    "        memory_info = [\n",
    "            [\"Total Memory (MB)\", total_memory / (1024**2)],\n",
    "            [\"Reserved Memory (MB)\", reserved_memory / (1024**2)],\n",
    "            [\"Allocated Memory (MB)\", allocated_memory / (1024**2)],\n",
    "            [\"Free Memory (MB)\", free_memory / (1024**2)]\n",
    "        ]\n",
    "\n",
    "        # Formatting the memory info in a table\n",
    "        print(tabulate(memory_info, headers=[\"Metric\", \"Value (MB)\"], tablefmt=\"pretty\"))\n",
    "    else:\n",
    "        print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a812b402-17b6-4177-b1c7-00ea0dcad346",
   "metadata": {},
   "source": [
    "# Model inference with HuggingFace transformers \n",
    "\n",
    "We will load a certain model from HuggingFace and run inference on it.\n",
    "\n",
    "The example models below are all finetuned versions of the first one, Mistral 7B. Which is a pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c016132-843b-4467-925c-f3b0a9197617",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc97afa4-4e43-4ee2-b7cb-50b7229654bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/raul/mambaforge/envs/llm_slides/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:20<00:00, 10.11s/it]\n",
      "/shared/raul/mambaforge/envs/llm_slides/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------------+\n",
      "|        Metric         |  Value (MB)   |\n",
      "+-----------------------+---------------+\n",
      "|   Total Memory (MB)   |  24217.3125   |\n",
      "| Reserved Memory (MB)  |    13946.0    |\n",
      "| Allocated Memory (MB) | 13940.5546875 |\n",
      "|   Free Memory (MB)    |  10271.3125   |\n",
      "+-----------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "#model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "#model_name = \"monology/openinstruct-mistral-7b\"\n",
    "model_name = \"openchat/openchat_3.5\"\n",
    "#model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# If the type is not specified to bfloat16, the model is loaded in float32 and takes twice the memory\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"cuda\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "get_gpu_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3948a92e-b9c6-41d0-87bb-595f78ecdac5",
   "metadata": {},
   "source": [
    "#### The model we got is \"just\" a regular pytorch nn.Model with a bunch of layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce2cccd1-e1b3-403d-a42a-75546ccd0385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32002, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badc0ec5-b1d8-4536-a1e2-8a6b64645953",
   "metadata": {},
   "source": [
    "## Prepare inputs for the model using the tokenizer\n",
    "\n",
    "LLMs expect \"tokens\" as inputs.\n",
    "A token is the number representation of a group of characters according to a \"vocabulary\".\n",
    "Might be a letter, symbol, word, emoji..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef62b8d-4873-49f3-af65-9c5029c2c42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Vocabulary size: 32002"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "vocab = tokenizer.get_vocab()\n",
    "display(Markdown(f\"### Vocabulary size: {len(vocab)}\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a66cd68-4ff4-41c5-8a79-b90c6af94d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## First tokens in the vocabulary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>: 0\n",
      "<s>: 1\n",
      "</s>: 2\n",
      "<0x00>: 3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Tokens for the first letters"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 28708\n",
      "b: 28726\n",
      "c: 28717\n",
      "d: 28715\n",
      "e: 28706\n",
      "f: 28722\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"## First tokens in the vocabulary\"))\n",
    "print(\"\\n\".join([f\"{k}: {v}\" for k,v in dict(sorted(vocab.items(), key=lambda k: k[1])).items()][:4]))\n",
    "display(Markdown(\"## Tokens for the first letters\"))\n",
    "print(\"\\n\".join([f\"{k}: {v}\" for k,v in dict(sorted(vocab.items())).items() if len(k)==1 and k in \"abcdef\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f07ec-ba18-484b-8199-09073c1f5ef0",
   "metadata": {},
   "source": [
    "### **Original message**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "286d31d0-ac40-40d3-a0df-cd9531688859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**System prompt:** You are a helpful assistant.\n",
       "\n",
       "ðŸ‘¤: Translate 'The lazy dog' to spanish.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"Translate 'The lazy dog' to spanish.\"\"\"},\n",
    "]\n",
    "display_chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5427f36-c3b8-4203-95a1-e6af7e9e1829",
   "metadata": {},
   "source": [
    "## Some message is translated into a list of tokens\n",
    "### **Raw text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af95a811-f98a-4d63-8f1d-7e9a0367f271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>GPT4 Correct System: You are a helpful assistant.<|end_of_turn|>GPT4 Correct User: Translate 'The lazy dog' to spanish.<|end_of_turn|>GPT4 Correct Assistant:\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33519a8-ebec-45a7-a4ff-9f8db8c53bfe",
   "metadata": {},
   "source": [
    "### **Tokens for the model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6b45899-2e1c-4a67-be92-701dfb898007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,   420,  6316, 28781,  3198,  3123,  2135, 28747,   995,   460,\n",
      "          264, 10865, 13892, 28723, 32000,   420,  6316, 28781,  3198,  3123,\n",
      "         1247, 28747,  4335, 10020,   464,  1014, 17898,  3914, 28742,   298,\n",
      "        12363,   789, 28723, 32000,   420,  6316, 28781,  3198,  3123, 21631,\n",
      "        28747], device='cuda:0')\n",
      "['<s>', 'G', 'PT', '4', 'Cor', 'rect', 'System', ':', 'You', 'are', 'a', 'helpful', 'assistant', '.', '<|end_of_turn|>', 'G', 'PT', '4', 'Cor', 'rect', 'User', ':', 'Trans', 'late', \"'\", 'The', 'lazy', 'dog', \"'\", 'to', 'span', 'ish', '.', '<|end_of_turn|>', 'G', 'PT', '4', 'Cor', 'rect', 'Assistant', ':']\n"
     ]
    }
   ],
   "source": [
    "inputs = (\n",
    "    tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, tokenize=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    .to(\"cuda\")\n",
    ")\n",
    "print(inputs[0])\n",
    "print([tokenizer.decode(s) for s in inputs[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf6995b-59fe-47c4-ad7f-a4ef3e1227d8",
   "metadata": {},
   "source": [
    "## Now lets run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2312c60-b596-4fcd-b4e9-2309643aee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**System prompt:** You are a helpful assistant.\n",
       "\n",
       "ðŸ‘¤: Translate 'The lazy dog' to spanish.\n",
       "\n",
       "ðŸ¤–: The translation of 'The lazy dog' to Spanish is 'El perro perezoso'.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_tokens = model.generate(inputs, max_new_tokens=100)[0][inputs.shape[-1] :]\n",
    "response = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "display_chat(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
